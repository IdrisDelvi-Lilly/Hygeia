{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install semantic-kernel==1.0.4\n",
    "!python -m pip install azure-core==1.30.1\n",
    "!python -m pip install azure-search-documents==11.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load values from your .env file.  Rename the notebook.env to .env and add your values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "embedding_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aiSerchEndpoint =os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\")\n",
    "aiSearchKey = os.getenv(\"AZURE_AI_SEARCH_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai.open_ai.services.azure_text_embedding import AzureTextEmbedding\n",
    "from semantic_kernel.core_plugins.text_memory_plugin import TextMemoryPlugin\n",
    "from semantic_kernel.kernel import Kernel\n",
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\n",
    "kernel = Kernel()\n",
    "\n",
    "service_id = \"manuallookup\"\n",
    "\n",
    "\n",
    "azure_chat_service = AzureChatCompletion(\n",
    "                        service_id=service_id,\n",
    "                        deployment_name=deployment,\n",
    "                        endpoint=endpoint,\n",
    "                        api_key=api_key,\n",
    "                    )\n",
    "\n",
    "embedding_gen = AzureTextEmbedding(service_id=\"embedding\", deployment_name=embedding_deployment)\n",
    "kernel.add_service(azure_chat_service)\n",
    "kernel.add_service(embedding_gen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.memory.azure_cognitive_search import AzureCognitiveSearchMemoryStore\n",
    "\n",
    "acs_memory_store = AzureCognitiveSearchMemoryStore( vector_size=1536, search_endpoint=aiSerchEndpoint , admin_key=aiSearchKey)\n",
    "\n",
    "memory = SemanticTextMemory(storage=acs_memory_store, embeddings_generator=embedding_gen)\n",
    "aiSearchPlugin = kernel.add_plugin(TextMemoryPlugin(memory), plugin_name=\"AISearch\")\n",
    "aiSearch = aiSearchPlugin[\"recall\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kmmanuals is the index within AI Search created by the Kernel Memory Service.\n",
    ">Documents are the french manuals from the user story\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await aiSearch.invoke(kernel, ask=\"How do I prepare the RRU\", collection='kmmanuals')\n",
    "print(f\"Answer: {result[0].text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.prompt_template import InputVariable, PromptTemplateConfig\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatPromptExecutionSettings\n",
    "\n",
    "prompt = \"\"\"\n",
    "                          You are an AI assistant that helps machine operators ask questions from manuals.  \n",
    "                            The questions could be in English or French, and the documents are currently in French. \n",
    "                            Find the answer to the question.\n",
    "                            TRANSLATE THE ANSWER TO THE LANGUAGE THE QUESTION IS ASKED\n",
    "\n",
    "                            ###\n",
    "                            How can I replace the silicon hose?\n",
    "\n",
    "                            To change the silicon hose follow these steps:\n",
    "                            1. Remove the hose\n",
    "                            2. replace the hose\n",
    "                            3.tighten the hose\n",
    "                            4. restart the machine\n",
    "\n",
    "                          Question: {{$input}}\n",
    "                          Tool call result: {{AISearch.recall $input collection='kmmanuals'}}\n",
    "                          If the answer is empty say \"I don't know\", otherwise reply with a preview of the answer.\n",
    "                          \"\"\"\n",
    "# AISearch.recall $input collection='machinestest' relevance=0 limit=1\n",
    "\n",
    "        \n",
    "execution_settings = OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        ai_model_id=deployment,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"operator\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"input\", description=\"The question asked\", is_required=True),\n",
    "        #InputVariable(name=\"index\", description=\"The index of AI Search\", is_required=True)\n",
    "    ],\n",
    "    execution_settings=execution_settings\n",
    ")\n",
    "\n",
    "\n",
    "op_query = kernel.add_function(\n",
    "    prompt=prompt,\n",
    "    plugin_name=\"OperatorsPlugIn\",\n",
    "    function_name=\"OperatorsQuery\",\n",
    "    prompt_template_config=prompt_template_config\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do I prepare the RRU\"\n",
    "print(f\"Question: {question}\")\n",
    "result = await op_query.invoke(kernel, input=question)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
